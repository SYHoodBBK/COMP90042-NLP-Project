{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652ed19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n",
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afdfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1228 training claims.\n",
      "Loaded 154 dev claims.\n",
      "Loaded 1208827 evidences.\n"
     ]
    }
   ],
   "source": [
    "train_claims_path = 'data/train-claims.json'\n",
    "dev_claims_path = 'data/dev-claims.json'\n",
    "evidence_path = 'data/evidence.json'\n",
    "\n",
    "# loading the data\n",
    "with open(train_claims_path, 'r', encoding='utf-8') as f:\n",
    "    train_claims = json.load(f)\n",
    "with open(dev_claims_path, 'r', encoding='utf-8') as f:\n",
    "    dev_claims = json.load(f)\n",
    "with open(evidence_path, 'r', encoding='utf-8') as f:\n",
    "    evidences = json.load(f)\n",
    "\n",
    "# Extract the first 10 claims from the evidence data\n",
    "# evidences = dict(list(evidences.items())[:20000])\n",
    "\n",
    "print(f\"Loaded {len(train_claims)} training claims.\")\n",
    "print(f\"Loaded {len(dev_claims)} dev claims.\")\n",
    "print(f\"Loaded {len(evidences)} evidences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input text by:\n",
    "    - Removing non-ASCII characters\n",
    "    - Lowercasing\n",
    "    - Removing extra spaces\n",
    "    - Removing unwanted punctuations (optional, adjustable)\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Normalize unicode characters (e.g., replace \\u2019 with real symbols)\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    # Encode to ASCII bytes, ignore non-ASCII characters, then decode back to string\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove unwanted punctuations (keep basic ones if needed)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def build_inverted_index(evidences):\n",
    "    \"\"\"\n",
    "    Build:\n",
    "    1. inverted index: word -> set of evidence IDs\n",
    "    2. word sets for each evidence (not just cleaned text)\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    import re\n",
    "\n",
    "    inverted_index = defaultdict(set)\n",
    "    evidence_word_sets = {}\n",
    "\n",
    "    for evidence_id, text in evidences.items():\n",
    "        clean_text_ = clean_text(text)\n",
    "        word_list = re.findall(r'\\w+', clean_text_.lower())\n",
    "        word_set = set(word_list)\n",
    "\n",
    "        # Save set of words directly\n",
    "        evidence_word_sets[evidence_id] = word_set\n",
    "\n",
    "        # Build inverted index\n",
    "        for word in word_set:\n",
    "            inverted_index[word].add(evidence_id)\n",
    "\n",
    "    return inverted_index, evidence_word_sets\n",
    "\n",
    "#     return top_evidence_ids\n",
    "\n",
    "def simple_retrieve_fast(claim_text, inverted_index, evidence_word_sets, top_k=5, max_candidates=10000):\n",
    "    \"\"\"\n",
    "    Fast retrieval using inverted index + limited candidates.\n",
    "    Uses preprocessed evidence word sets directly.\n",
    "\n",
    "    Args:\n",
    "        claim_text (str): The claim text.\n",
    "        inverted_index (dict): Prebuilt word -> evidence_ids map.\n",
    "        evidence_word_sets (dict): {evidence_id: set of words}.\n",
    "        top_k (int): Number of evidences to retrieve.\n",
    "        max_candidates (int): Max number of evidence candidates to consider.\n",
    "\n",
    "    Returns:\n",
    "        List of evidence IDs.\n",
    "    \"\"\"\n",
    "    clean_claim = clean_text(claim_text)\n",
    "    claim_words = re.findall(r'\\w+', clean_claim.lower())\n",
    "    claim_word_set = set(claim_words)\n",
    "\n",
    "    # Gather candidate evidence IDs\n",
    "    candidate_ids = set()\n",
    "    for word in claim_word_set:\n",
    "        if word in inverted_index:\n",
    "            candidate_ids.update(inverted_index[word])\n",
    "\n",
    "    # Truncate to max_candidates\n",
    "    if len(candidate_ids) > max_candidates:\n",
    "        candidate_ids = set(list(candidate_ids)[:max_candidates])\n",
    "\n",
    "    # Fine-grained scoring\n",
    "    scores = []\n",
    "    for evidence_id in candidate_ids:\n",
    "        evidence_words = evidence_word_sets[evidence_id]\n",
    "        common_word_count = len(claim_word_set & evidence_words)\n",
    "        scores.append((evidence_id, common_word_count))\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_evidence_ids = [evidence_id for evidence_id, score in scores[:top_k]]\n",
    "\n",
    "    return top_evidence_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f204c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index and evidence word sets built.\n",
      "Number of unique words in inverted index: 568713\n",
      "Number of unique evidence IDs: 1208827\n"
     ]
    }
   ],
   "source": [
    "inverted_index, evidence_word_sets = build_inverted_index(evidences)\n",
    "print(\"Inverted index and evidence word sets built.\")\n",
    "print(f\"Number of unique words in inverted index: {len(inverted_index)}\")\n",
    "print(f\"Number of unique evidence IDs: {len(evidence_word_sets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b64999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example claim ID: claim-1937\n",
      "Example claim text: Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
      "Example cleaned claim text: not only is there no scientific evidence that co2 is a pollutant higher co2 concentrations actually help ecosystems support more plant and animal life\n",
      "Example cleaned evidence: john bennet lawes english entrepreneur and agricultural scientist\n",
      "Retrieved evidence IDs: ['evidence-367539', 'evidence-20656', 'evidence-888023', 'evidence-499734', 'evidence-832031']\n",
      "Retrieved evidence texts: ['omnivores also consume both animal and non animal food and apart from the more general definition there is no clearly defined ratio of plant to animal material that would distinguish a facultative carnivore from an omnivore', 'james huneker remarked that the four movements of the sonata have no common life and that the sonata is not more a sonata than it is a sequence of ballades and scherzi', 'a common argument used to dismiss the significance of human caused climate change is to allege that scientists showed concerns about global cooling which did not materialise and there is therefore no need to heed current scientific concerns about global warming', 'while the full implications of elevated co2 on marine ecosystems are still being documented there is a substantial body of research showing that a combination of ocean acidification and elevated ocean temperature driven mainly by co2 and other greenhouse gas emissions have a compounded effect on marine life and the ocean environment', 'mound no 2 is a flat topped circle measuring 48 ft by 4 5 ft a depression similar to the one in no 1 is found atop mound no 3 a circle measuring 39 ft by 2 ft and the only mound composed partly of rock surveyors in the 1960s deemed the depression evidence that there had formerly been a structure within the mound that had collapsed']\n"
     ]
    }
   ],
   "source": [
    "claim_id = list(train_claims)[0]\n",
    "print(f\"Example claim ID: {claim_id}\")\n",
    "claim_text = train_claims[claim_id][\"claim_text\"]\n",
    "print(f\"Example claim text: {claim_text}\")\n",
    "cleaned_claim_text = clean_text(claim_text)\n",
    "print(f\"Example cleaned claim text: {cleaned_claim_text}\")\n",
    "cleaned_evidences = {evidence_id: clean_text(evidence_text) for evidence_id, evidence_text in evidences.items()}\n",
    "print(f\"Example cleaned evidence: {list(cleaned_evidences.values())[0]}\")\n",
    "retrieved_evidence_ids = simple_retrieve_fast(cleaned_claim_text, inverted_index, evidence_word_sets, top_k=5, max_candidates=100000)\n",
    "print(f\"Retrieved evidence IDs: {retrieved_evidence_ids}\")\n",
    "print(f\"Retrieved evidence texts: {[cleaned_evidences[evidence_id] for evidence_id in retrieved_evidence_ids]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7af14220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the SimpleClassifier model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of the embedding vectors.\n",
    "            hidden_dim (int): Dimension of the hidden LSTM state.\n",
    "            output_dim (int): Number of output classes.\n",
    "        \"\"\"\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (Tensor): Tensor of token ids with shape (batch_size, seq_length).\n",
    "\n",
    "        Returns:\n",
    "            logits (Tensor): Raw output scores for each class (before softmax).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_ids)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        _, (hidden, _) = self.lstm(embedded)   # hidden shape: (1, batch_size, hidden_dim)\n",
    "        hidden = hidden.squeeze(0)             # Shape: (batch_size, hidden_dim)\n",
    "        logits = self.fc(hidden)                # Shape: (batch_size, output_dim)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "961ec325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the SimpleTokenizer.\n",
    "        \"\"\"\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "\n",
    "    def build_vocab(self, texts, min_freq=1):\n",
    "        \"\"\"\n",
    "        Build vocabulary from a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts (list): List of text strings.\n",
    "            min_freq (int): Minimum frequency a word must have to be included.\n",
    "        \"\"\"\n",
    "        word_freq = {}\n",
    "\n",
    "        for text in texts:\n",
    "            words = re.findall(r'\\w+', text.lower())\n",
    "            for word in words:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        # Initialize vocabulary with special tokens\n",
    "        self.word2id = {\n",
    "            self.pad_token: 0,\n",
    "            self.unk_token: 1\n",
    "        }\n",
    "        self.id2word = {\n",
    "            0: self.pad_token,\n",
    "            1: self.unk_token\n",
    "        }\n",
    "\n",
    "        idx = 2  # Start indexing words from 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.word2id[word] = idx\n",
    "                self.id2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode a text string into a list of token ids.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: List of token ids.\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\w+', text.lower())\n",
    "        ids = [self.word2id.get(word, self.word2id[self.unk_token]) for word in words]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token ids back into a text string.\n",
    "\n",
    "        Args:\n",
    "            ids (list): List of token ids.\n",
    "\n",
    "        Returns:\n",
    "            str: Decoded text string.\n",
    "        \"\"\"\n",
    "        words = [self.id2word.get(id_, self.unk_token) for id_ in ids]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "        Get the size of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of tokens in the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cff5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data_fast(claims, inverted_index, evidence_word_sets, tokenizer, top_k=5, max_seq_length=128, max_candidates=1000):\n",
    "    \"\"\"\n",
    "    Prepare the training dataset using fast retrieval with preprocessed word sets.\n",
    "\n",
    "    Args:\n",
    "        claims (dict): Dictionary of claim instances.\n",
    "        inverted_index (dict): Prebuilt word -> evidence_ids map.\n",
    "        evidence_word_sets (dict): Preprocessed word sets for each evidence.\n",
    "        tokenizer (SimpleTokenizer): The tokenizer to encode texts.\n",
    "        top_k (int): Number of evidences to retrieve per claim.\n",
    "        max_seq_length (int): Maximum length of input sequence.\n",
    "        max_candidates (int): Max candidate evidences for filtering.\n",
    "\n",
    "    Returns:\n",
    "        List of (input_ids, label_id) pairs.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    label_map = {\n",
    "        \"SUPPORTS\": 0,\n",
    "        \"REFUTES\": 1,\n",
    "        \"NOT_ENOUGH_INFO\": 2,\n",
    "        \"DISPUTED\": 3\n",
    "    }\n",
    "\n",
    "    for claim_id, claim_data in claims.items():\n",
    "        claim_text = claim_data['claim_text']\n",
    "        claim_label = claim_data['claim_label']\n",
    "\n",
    "        clean_claim = clean_text(claim_text)\n",
    "\n",
    "        # Retrieve top evidence IDs\n",
    "        retrieved_ids = simple_retrieve_fast(\n",
    "            clean_claim,\n",
    "            inverted_index,\n",
    "            evidence_word_sets,\n",
    "            top_k=top_k,\n",
    "            max_candidates=max_candidates\n",
    "        )\n",
    "\n",
    "        # Convert evidence word sets back to text (space-joined)\n",
    "        evidence_texts = [\n",
    "            \" \".join(sorted(evidence_word_sets[ev_id])) for ev_id in retrieved_ids if ev_id in evidence_word_sets\n",
    "        ]\n",
    "        combined_text = clean_claim + \" \" + \" \".join(evidence_texts)\n",
    "        combined_text = clean_text(combined_text)\n",
    "\n",
    "        # Encode\n",
    "        input_ids = tokenizer.encode(combined_text)\n",
    "\n",
    "        # Pad or truncate\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "        else:\n",
    "            padding_length = max_seq_length - len(input_ids)\n",
    "            input_ids = input_ids + [tokenizer.word2id[tokenizer.pad_token]] * padding_length\n",
    "\n",
    "        # Label\n",
    "        label_id = label_map.get(claim_label, 2)  # Default to NOT_ENOUGH_INFO\n",
    "\n",
    "        data.append((input_ids, label_id))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b77bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Custom Dataset for claim data.\n",
    "\n",
    "        Args:\n",
    "            data (list): List of (input_ids, label_id) pairs.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, label = self.data[idx]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9857471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_dataset, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The classification model.\n",
    "        val_dataset (Dataset): Validation dataset.\n",
    "        batch_size (int): Batch size.\n",
    "        device (str): 'cpu' or 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        avg_loss (float): Average loss on validation set.\n",
    "        accuracy (float): Accuracy on validation set.\n",
    "    \"\"\"\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Turn off gradient tracking\n",
    "        for batch in val_loader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train_model_with_validation(model, train_dataset, val_dataset, epochs=5, batch_size=16, learning_rate=1e-3, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the model with validation after each epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The classification model.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        val_dataset (Dataset): Validation dataset.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (int): Batch size.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        device (str): 'cpu' or 'cuda'.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to train mode\n",
    "\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_dataset, batch_size, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "057701d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect all texts for building vocabulary\n",
    "texts_for_vocab = []\n",
    "\n",
    "for claim in train_claims.values():\n",
    "    texts_for_vocab.append(claim['claim_text'])\n",
    "for ev in evidences.values():\n",
    "    texts_for_vocab.append(ev)\n",
    "\n",
    "# Step 2: Create and build tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(texts_for_vocab)\n",
    "\n",
    "# Step 3: initialize the model\n",
    "model = SimpleClassifier(\n",
    "    vocab_size=tokenizer.vocab_size(), \n",
    "    embedding_dim=128, \n",
    "    hidden_dim=256, \n",
    "    output_dim=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f41999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Prepared 1228 training samples and 154 validation samples.\n",
      "Training dataset size: 1228\n",
      "Validation dataset size: 154\n"
     ]
    }
   ],
   "source": [
    "# Move to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare train and validation datasets\n",
    "train_data = prepare_training_data_fast(train_claims, inverted_index, evidence_word_sets, tokenizer, top_k=5, max_seq_length=128, max_candidates=100000)\n",
    "val_data = prepare_training_data_fast(dev_claims, inverted_index, evidence_word_sets, tokenizer, top_k=5, max_seq_length=128, max_candidates=100000)\n",
    "print(f\"Prepared {len(train_data)} training samples and {len(val_data)} validation samples.\")\n",
    "\n",
    "train_dataset = ClaimDataset(train_data)\n",
    "val_dataset = ClaimDataset(val_data)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "711c8c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 1.5758 | Train Acc: 0.3550 | Val Loss: 1.3378 | Val Acc: 0.4286\n",
      "Epoch 2/5 | Train Loss: 1.1711 | Train Acc: 0.4593 | Val Loss: 1.3964 | Val Acc: 0.3571\n",
      "Epoch 3/5 | Train Loss: 1.0179 | Train Acc: 0.5619 | Val Loss: 1.5263 | Val Acc: 0.3247\n",
      "Epoch 4/5 | Train Loss: 0.9011 | Train Acc: 0.6124 | Val Loss: 1.5658 | Val Acc: 0.3831\n",
      "Epoch 5/5 | Train Loss: 0.7952 | Train Acc: 0.6555 | Val Loss: 1.7516 | Val Acc: 0.3312\n"
     ]
    }
   ],
   "source": [
    "# Start training with validation\n",
    "train_model_with_validation(model, train_dataset, val_dataset, epochs=5, batch_size=16, learning_rate=1e-3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(\n",
    "    model,\n",
    "    test_claims,\n",
    "    inverted_index,\n",
    "    evidence_word_sets,\n",
    "    tokenizer,\n",
    "    output_file=\"test-output.json\",\n",
    "    top_k=5,\n",
    "    max_seq_length=128,\n",
    "    max_candidates=1000,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    model.eval()\n",
    "    id_to_label = {\n",
    "        0: \"SUPPORTS\",\n",
    "        1: \"REFUTES\",\n",
    "        2: \"NOT_ENOUGH_INFO\",\n",
    "        3: \"DISPUTED\"\n",
    "    }\n",
    "\n",
    "    output_dict = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for claim_id, claim_data in test_claims.items():\n",
    "            claim_text = claim_data[\"claim_text\"]\n",
    "\n",
    "            # Retrieve top evidence IDs\n",
    "            retrieved_ids = simple_retrieve_fast(\n",
    "                claim_text,\n",
    "                inverted_index,\n",
    "                evidence_word_sets,\n",
    "                top_k=top_k,\n",
    "                max_candidates=max_candidates\n",
    "            )\n",
    "\n",
    "            # Join evidence words (for input encoding)\n",
    "            evidence_texts = [\n",
    "                \" \".join(sorted(evidence_word_sets[ev_id])) for ev_id in retrieved_ids if ev_id in evidence_word_sets\n",
    "            ]\n",
    "            combined_text = clean_text(claim_text) + \" \" + \" \".join(evidence_texts)\n",
    "            input_ids = tokenizer.encode(combined_text)\n",
    "\n",
    "            # Pad/truncate\n",
    "            if len(input_ids) > max_seq_length:\n",
    "                input_ids = input_ids[:max_seq_length]\n",
    "            else:\n",
    "                padding = max_seq_length - len(input_ids)\n",
    "                input_ids += [tokenizer.word2id[tokenizer.pad_token]] * padding\n",
    "\n",
    "            # Predict\n",
    "            input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "            output = model(input_tensor)\n",
    "            pred_label_id = torch.argmax(output, dim=1).item()\n",
    "            pred_label = id_to_label[pred_label_id]\n",
    "\n",
    "            # Construct output\n",
    "            output_dict[claim_id] = {\n",
    "                \"claim_text\": claim_text,\n",
    "                \"claim_label\": pred_label,\n",
    "                \"evidences\": retrieved_ids  # top_k evidences\n",
    "            }\n",
    "\n",
    "    # Save to JSON\n",
    "    import json\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(output_dict, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Submission file saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53df6660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission file saved to: submission.json\n"
     ]
    }
   ],
   "source": [
    "test_path = 'data/test-claims-unlabelled.json'\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test_claims = json.load(f)\n",
    "\n",
    "generate_submission(\n",
    "    model=model,\n",
    "    test_claims=test_claims,\n",
    "    inverted_index=inverted_index,\n",
    "    evidence_word_sets=evidence_word_sets,\n",
    "    tokenizer=tokenizer,\n",
    "    output_file=\"submission.json\",\n",
    "    device=\"cuda\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
