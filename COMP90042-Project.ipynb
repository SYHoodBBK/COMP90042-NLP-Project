{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652ed19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n",
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00afdfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1228 training claims.\n",
      "Loaded 154 dev claims.\n",
      "Loaded 1208827 evidences.\n"
     ]
    }
   ],
   "source": [
    "train_claims_path = 'data/train-claims.json'\n",
    "dev_claims_path = 'data/dev-claims.json'\n",
    "evidence_path = 'data/evidence.json'\n",
    "\n",
    "# loading the data\n",
    "with open(train_claims_path, 'r', encoding='utf-8') as f:\n",
    "    train_claims = json.load(f)\n",
    "with open(dev_claims_path, 'r', encoding='utf-8') as f:\n",
    "    dev_claims = json.load(f)\n",
    "with open(evidence_path, 'r', encoding='utf-8') as f:\n",
    "    evidences = json.load(f)\n",
    "\n",
    "# Extract the first 10 claims from the evidence data\n",
    "# evidences = dict(list(evidences.items())[:20000])\n",
    "\n",
    "print(f\"Loaded {len(train_claims)} training claims.\")\n",
    "print(f\"Loaded {len(dev_claims)} dev claims.\")\n",
    "print(f\"Loaded {len(evidences)} evidences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4df8d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input text by:\n",
    "    - Removing non-ASCII characters\n",
    "    - Lowercasing\n",
    "    - Removing extra spaces\n",
    "    - Removing unwanted punctuations (optional, adjustable)\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    # Normalize unicode characters (e.g., replace \\u2019 with real symbols)\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    # Encode to ASCII bytes, ignore non-ASCII characters, then decode back to string\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove unwanted punctuations (keep basic ones if needed)\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def build_inverted_index(evidences):\n",
    "    \"\"\"\n",
    "    Build an inverted index mapping word -> set of evidence IDs.\n",
    "\n",
    "    Args:\n",
    "        evidences (dict): {evidence_id: evidence_text}\n",
    "\n",
    "    Returns:\n",
    "        inverted_index (dict): {word: set of evidence_ids}\n",
    "        cleaned_evidences (dict): {evidence_id: cleaned_text}\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(set)\n",
    "    cleaned_evidences = {}\n",
    "\n",
    "    for evidence_id, text in evidences.items():\n",
    "        clean_text_ = clean_text(text)\n",
    "        cleaned_evidences[evidence_id] = clean_text_\n",
    "        words = re.findall(r'\\w+', clean_text_.lower())\n",
    "        for word in set(words):  # use set to avoid duplicates\n",
    "            inverted_index[word].add(evidence_id)\n",
    "\n",
    "    return inverted_index, cleaned_evidences\n",
    "\n",
    "# def simple_retrieve(claim_text, evidences, top_k=5):\n",
    "#     \"\"\"\n",
    "#     Retrieve top_k relevant evidence passages for a given claim using simple word matching.\n",
    "\n",
    "#     Args:\n",
    "#         claim_text (str): The claim text to search for.\n",
    "#         evidences (dict): Dictionary mapping evidence ID to evidence text.\n",
    "#         top_k (int): Number of evidences to retrieve.\n",
    "\n",
    "#     Returns:\n",
    "#         List of evidence IDs.\n",
    "#     \"\"\"\n",
    "#     # Preprocess claim: lowercase and split into words\n",
    "#     claim_words = re.findall(r'\\w+', claim_text.lower())\n",
    "#     claim_word_set = set(claim_words)\n",
    "#     # print(f\"Claim words: {claim_word_set}\")\n",
    "\n",
    "#     scores = []\n",
    "#     for evidence_id, evidence_text in evidences.items():\n",
    "#         evidence_words = re.findall(r'\\w+', evidence_text.lower())\n",
    "#         evidence_word_set = set(evidence_words)\n",
    "\n",
    "#         # Count the number of common words\n",
    "#         common_word_count = len(claim_word_set & evidence_word_set)\n",
    "#         scores.append((evidence_id, common_word_count))\n",
    "\n",
    "#     # Sort by score (descending) and take top_k\n",
    "#     scores.sort(key=lambda x: x[1], reverse=True)\n",
    "#     # print(f\"Scores: {scores[:top_k]}\")\n",
    "#     top_evidence_ids = [evidence_id for evidence_id, score in scores[:top_k]]\n",
    "\n",
    "#     return top_evidence_ids\n",
    "\n",
    "def simple_retrieve_fast(claim_text, inverted_index, cleaned_evidences, top_k=5, max_candidates=1000000):\n",
    "    \"\"\"\n",
    "    Fast retrieval using inverted index + limited candidates.\n",
    "\n",
    "    Args:\n",
    "        claim_text (str): The claim text.\n",
    "        inverted_index (dict): Prebuilt word -> evidence_ids map.\n",
    "        cleaned_evidences (dict): Preprocessed evidence texts.\n",
    "        top_k (int): Number of evidences to retrieve.\n",
    "        max_candidates (int): Max number of evidence candidates to consider.\n",
    "\n",
    "    Returns:\n",
    "        List of evidence IDs.\n",
    "    \"\"\"\n",
    "    clean_claim = clean_text(claim_text)\n",
    "    claim_words = re.findall(r'\\w+', clean_claim.lower())\n",
    "\n",
    "    # Gather candidate evidence IDs based on claim words\n",
    "    candidate_ids = set()\n",
    "    for word in claim_words:\n",
    "        if word in inverted_index:\n",
    "            candidate_ids.update(inverted_index[word])\n",
    "\n",
    "    # Limit the number of candidates\n",
    "    if len(candidate_ids) > max_candidates:\n",
    "        candidate_ids = set(list(candidate_ids)[:max_candidates])\n",
    "\n",
    "    # Now do fine-grained matching\n",
    "    claim_word_set = set(claim_words)\n",
    "    scores = []\n",
    "    for evidence_id in candidate_ids:\n",
    "        evidence_words = set(re.findall(r'\\w+', cleaned_evidences[evidence_id].lower()))\n",
    "        common_word_count = len(claim_word_set & evidence_words)\n",
    "        scores.append((evidence_id, common_word_count))\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # print(f\"Scores: {scores[:top_k]}\")\n",
    "    top_evidence_ids = [evidence_id for evidence_id, score in scores[:top_k]]\n",
    "\n",
    "    return top_evidence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f204c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index, cleaned_evidences = build_inverted_index(evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b64999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example claim ID: claim-1937\n",
      "Example claim text: Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.\n",
      "Example cleaned claim text: not only is there no scientific evidence that co2 is a pollutant higher co2 concentrations actually help ecosystems support more plant and animal life\n",
      "Example cleaned evidence: john bennet lawes english entrepreneur and agricultural scientist\n",
      "Retrieved evidence IDs: ['evidence-10884', 'evidence-3648', 'evidence-17945', 'evidence-13434', 'evidence-7907']\n",
      "Retrieved evidence texts: ['parents feel a sense of shame and humiliation to have that problem so they rarely seek help and there is usually little or no help available anyway', 'the main point being that there is a conflict between the oecd states budget deficit cuts the need to help developing countries adapt to develop sustainably and the need to ensure that funding does not come from cutting aid to other important millennium development goals', 'a story told to tourists about the origins of the 1691 version of the bridge is that it was built by two wealthy sisters who lived on opposite sides of the amstel river and wanted to be able to visit one another every day and were presumably too busy or not in good enough health to go the long way round via another bridge of which there must surely have been at least one', 'there is also evidence that global warming is leading to increased precipitation to the eastern portions of north america while droughts are becoming more frequent in the tropics and subtropics', 'this does not only lead to poisonous chemicals to disperse on water runoff but also to the emission of nitrous oxide no as a fertilizer byproduct which is three hundred times more efficient in producing a greenhouse effect than carbon dioxide co']\n"
     ]
    }
   ],
   "source": [
    "claim_id = list(train_claims)[0]\n",
    "print(f\"Example claim ID: {claim_id}\")\n",
    "claim_text = train_claims[claim_id][\"claim_text\"]\n",
    "print(f\"Example claim text: {claim_text}\")\n",
    "cleaned_claim_text = clean_text(claim_text)\n",
    "print(f\"Example cleaned claim text: {cleaned_claim_text}\")\n",
    "cleaned_evidences = {evidence_id: clean_text(evidence_text) for evidence_id, evidence_text in evidences.items()}\n",
    "print(f\"Example cleaned evidence: {list(cleaned_evidences.values())[0]}\")\n",
    "retrieved_evidence_ids = simple_retrieve_fast(cleaned_claim_text, inverted_index, cleaned_evidences, top_k=5, max_candidates=100000)\n",
    "print(f\"Retrieved evidence IDs: {retrieved_evidence_ids}\")\n",
    "print(f\"Retrieved evidence texts: {[cleaned_evidences[evidence_id] for evidence_id in retrieved_evidence_ids]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7af14220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the SimpleClassifier model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            embedding_dim (int): Dimension of the embedding vectors.\n",
    "            hidden_dim (int): Dimension of the hidden LSTM state.\n",
    "            output_dim (int): Number of output classes.\n",
    "        \"\"\"\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_ids (Tensor): Tensor of token ids with shape (batch_size, seq_length).\n",
    "\n",
    "        Returns:\n",
    "            logits (Tensor): Raw output scores for each class (before softmax).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_ids)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        _, (hidden, _) = self.lstm(embedded)   # hidden shape: (1, batch_size, hidden_dim)\n",
    "        hidden = hidden.squeeze(0)             # Shape: (batch_size, hidden_dim)\n",
    "        logits = self.fc(hidden)                # Shape: (batch_size, output_dim)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "961ec325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the SimpleTokenizer.\n",
    "        \"\"\"\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.pad_token = \"<PAD>\"\n",
    "        self.unk_token = \"<UNK>\"\n",
    "\n",
    "    def build_vocab(self, texts, min_freq=1):\n",
    "        \"\"\"\n",
    "        Build vocabulary from a list of texts.\n",
    "\n",
    "        Args:\n",
    "            texts (list): List of text strings.\n",
    "            min_freq (int): Minimum frequency a word must have to be included.\n",
    "        \"\"\"\n",
    "        word_freq = {}\n",
    "\n",
    "        for text in texts:\n",
    "            words = re.findall(r'\\w+', text.lower())\n",
    "            for word in words:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "        # Initialize vocabulary with special tokens\n",
    "        self.word2id = {\n",
    "            self.pad_token: 0,\n",
    "            self.unk_token: 1\n",
    "        }\n",
    "        self.id2word = {\n",
    "            0: self.pad_token,\n",
    "            1: self.unk_token\n",
    "        }\n",
    "\n",
    "        idx = 2  # Start indexing words from 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                self.word2id[word] = idx\n",
    "                self.id2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode a text string into a list of token ids.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: List of token ids.\n",
    "        \"\"\"\n",
    "        words = re.findall(r'\\w+', text.lower())\n",
    "        ids = [self.word2id.get(word, self.word2id[self.unk_token]) for word in words]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token ids back into a text string.\n",
    "\n",
    "        Args:\n",
    "            ids (list): List of token ids.\n",
    "\n",
    "        Returns:\n",
    "            str: Decoded text string.\n",
    "        \"\"\"\n",
    "        words = [self.id2word.get(id_, self.unk_token) for id_ in ids]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def vocab_size(self):\n",
    "        \"\"\"\n",
    "        Get the size of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of tokens in the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f94462c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_training_data(claims, evidences, tokenizer, top_k=5, max_seq_length=128):\n",
    "#     \"\"\"\n",
    "#     Prepare the training dataset.\n",
    "\n",
    "#     Args:\n",
    "#         claims (dict): Dictionary of claim instances.\n",
    "#         evidences (dict): Dictionary of evidence passages.\n",
    "#         tokenizer (SimpleTokenizer): The tokenizer to encode texts.\n",
    "#         top_k (int): Number of evidences to retrieve per claim.\n",
    "#         max_seq_length (int): Maximum length of input sequence.\n",
    "\n",
    "#     Returns:\n",
    "#         List of (input_ids, label_id) pairs.\n",
    "#     \"\"\"\n",
    "#     data = []\n",
    "\n",
    "#     label_map = {\n",
    "#         \"SUPPORTS\": 0,\n",
    "#         \"REFUTES\": 1,\n",
    "#         \"NOT_ENOUGH_INFO\": 2,\n",
    "#         \"DISPUTED\": 3\n",
    "#     }\n",
    "\n",
    "#     for claim_id, claim_data in claims.items():\n",
    "#         # print(f\"Processing claim ID: {claim_id}\")\n",
    "#         claim_text = claim_data['claim_text']\n",
    "#         claim_label = claim_data['claim_label']\n",
    "\n",
    "#         # Clean the claim\n",
    "#         clean_claim = clean_text(claim_text)\n",
    "\n",
    "#         # Clean the evidences\n",
    "#         cleaned_evidences = {evidence_id: clean_text(evidence_text) for evidence_id, evidence_text in evidences.items()}\n",
    "\n",
    "#         # Retrieve evidences\n",
    "#         retrieved_ids = simple_retrieve(clean_claim, cleaned_evidences, top_k=top_k)\n",
    "\n",
    "#         # Concatenate claim and retrieved evidences\n",
    "#         evidence_texts = [cleaned_evidences[ev_id] for ev_id in retrieved_ids if ev_id in cleaned_evidences]\n",
    "#         combined_text = clean_claim + \" \" + \" \".join(evidence_texts)\n",
    "#         combined_text = clean_text(combined_text)\n",
    "\n",
    "#         # Encode to token ids\n",
    "#         input_ids = tokenizer.encode(combined_text)\n",
    "\n",
    "#         # Pad or truncate to max_seq_length\n",
    "#         if len(input_ids) > max_seq_length:\n",
    "#             input_ids = input_ids[:max_seq_length]\n",
    "#         else:\n",
    "#             padding_length = max_seq_length - len(input_ids)\n",
    "#             input_ids = input_ids + [tokenizer.word2id[tokenizer.pad_token]] * padding_length\n",
    "\n",
    "#         # Map claim label to an integer\n",
    "#         label_id = label_map.get(claim_label, 2)  # Default to NOT_ENOUGH_INFO if unknown\n",
    "\n",
    "#         data.append((input_ids, label_id))\n",
    "#         # print(f\"input_ids: {input_ids} label_id: {label_id}\")\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0cff5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(claims, inverted_index, cleaned_evidences, tokenizer, top_k=5, max_seq_length=128, max_candidates=100000):\n",
    "    \"\"\"\n",
    "    Prepare the training dataset using fast retrieval.\n",
    "\n",
    "    Args:\n",
    "        claims (dict): Dictionary of claim instances.\n",
    "        inverted_index (dict): Prebuilt inverted index.\n",
    "        cleaned_evidences (dict): Preprocessed evidence texts.\n",
    "        tokenizer (SimpleTokenizer): The tokenizer to encode texts.\n",
    "        top_k (int): Number of evidences to retrieve per claim.\n",
    "        max_seq_length (int): Maximum length of input sequence.\n",
    "        max_candidates (int): Maximum number of evidence candidates for retrieval.\n",
    "\n",
    "    Returns:\n",
    "        List of (input_ids, label_id) pairs.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    label_map = {\n",
    "        \"SUPPORTS\": 0,\n",
    "        \"REFUTES\": 1,\n",
    "        \"NOT_ENOUGH_INFO\": 2,\n",
    "        \"DISPUTED\": 3\n",
    "    }\n",
    "\n",
    "    for claim_id, claim_data in claims.items():\n",
    "        claim_text = claim_data['claim_text']\n",
    "        claim_label = claim_data['claim_label']\n",
    "\n",
    "        # Clean claim text\n",
    "        clean_claim = clean_text(claim_text)\n",
    "\n",
    "        # Fast retrieve evidences\n",
    "        retrieved_ids = simple_retrieve_fast(\n",
    "            clean_claim,\n",
    "            inverted_index,\n",
    "            cleaned_evidences,\n",
    "            top_k=top_k,\n",
    "            max_candidates=max_candidates\n",
    "        )\n",
    "        # print(f\"Claim ID: {claim_id} retrieved evidence IDs: {retrieved_ids}\")\n",
    "\n",
    "        # Concatenate claim and retrieved evidences\n",
    "        evidence_texts = [cleaned_evidences[ev_id] for ev_id in retrieved_ids if ev_id in cleaned_evidences]\n",
    "        combined_text = clean_claim + \" \" + \" \".join(evidence_texts)\n",
    "\n",
    "        # Final clean\n",
    "        combined_text = clean_text(combined_text)\n",
    "\n",
    "        # Encode\n",
    "        input_ids = tokenizer.encode(combined_text)\n",
    "\n",
    "        # Pad or truncate\n",
    "        if len(input_ids) > max_seq_length:\n",
    "            input_ids = input_ids[:max_seq_length]\n",
    "        else:\n",
    "            padding_length = max_seq_length - len(input_ids)\n",
    "            input_ids = input_ids + [tokenizer.word2id[tokenizer.pad_token]] * padding_length\n",
    "\n",
    "        # Label\n",
    "        label_id = label_map.get(claim_label, 2)  # Default to NOT_ENOUGH_INFO if unknown\n",
    "\n",
    "        data.append((input_ids, label_id))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b77bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Custom Dataset for claim data.\n",
    "\n",
    "        Args:\n",
    "            data (list): List of (input_ids, label_id) pairs.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, label = self.data[idx]\n",
    "        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9857471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_dataset, batch_size=16, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The classification model.\n",
    "        val_dataset (Dataset): Validation dataset.\n",
    "        batch_size (int): Batch size.\n",
    "        device (str): 'cpu' or 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        avg_loss (float): Average loss on validation set.\n",
    "        accuracy (float): Accuracy on validation set.\n",
    "    \"\"\"\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Turn off gradient tracking\n",
    "        for batch in val_loader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def train_model_with_validation(model, train_dataset, val_dataset, epochs=5, batch_size=16, learning_rate=1e-3, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the model with validation after each epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The classification model.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        val_dataset (Dataset): Validation dataset.\n",
    "        epochs (int): Number of epochs.\n",
    "        batch_size (int): Batch size.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        device (str): 'cpu' or 'cuda'.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to train mode\n",
    "\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_dataset, batch_size, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "057701d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect all texts for building vocabulary\n",
    "texts_for_vocab = []\n",
    "\n",
    "for claim in train_claims.values():\n",
    "    texts_for_vocab.append(claim['claim_text'])\n",
    "for ev in evidences.values():\n",
    "    texts_for_vocab.append(ev)\n",
    "\n",
    "# Step 2: Create and build tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(texts_for_vocab)\n",
    "\n",
    "# Step 3: initialize the model\n",
    "model = SimpleClassifier(\n",
    "    vocab_size=tokenizer.vocab_size(), \n",
    "    embedding_dim=128, \n",
    "    hidden_dim=256, \n",
    "    output_dim=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76f41999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Prepared 1228 training samples and 154 validation samples.\n",
      "Training dataset size: 1228\n",
      "Validation dataset size: 154\n",
      "Epoch 1/5 | Train Loss: 1.2808 | Train Acc: 0.4088 | Val Loss: 1.2766 | Val Acc: 0.4481\n",
      "Epoch 2/5 | Train Loss: 1.1516 | Train Acc: 0.4829 | Val Loss: 1.2646 | Val Acc: 0.4156\n",
      "Epoch 3/5 | Train Loss: 0.9872 | Train Acc: 0.5985 | Val Loss: 1.3581 | Val Acc: 0.4286\n",
      "Epoch 4/5 | Train Loss: 0.7314 | Train Acc: 0.7207 | Val Loss: 1.4631 | Val Acc: 0.4156\n",
      "Epoch 5/5 | Train Loss: 0.4643 | Train Acc: 0.8461 | Val Loss: 1.7873 | Val Acc: 0.3896\n"
     ]
    }
   ],
   "source": [
    "# Move to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare train and validation datasets\n",
    "train_data = prepare_training_data(train_claims, inverted_index, cleaned_evidences, tokenizer, top_k=5, max_seq_length=128)\n",
    "val_data = prepare_training_data(dev_claims, inverted_index, cleaned_evidences, tokenizer, top_k=5, max_seq_length=128)\n",
    "print(f\"Prepared {len(train_data)} training samples and {len(val_data)} validation samples.\")\n",
    "\n",
    "train_dataset = ClaimDataset(train_data)\n",
    "val_dataset = ClaimDataset(val_data)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Start training with validation\n",
    "train_model_with_validation(model, train_dataset, val_dataset, epochs=5, batch_size=16, learning_rate=1e-3, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
