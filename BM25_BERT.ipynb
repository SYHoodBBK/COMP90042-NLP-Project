{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af319155",
    "language": "markdown"
   },
   "source": [
    "# 搜索算法流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "724b81fb",
    "language": "markdown"
   },
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df061311",
   "metadata": {
    "id": "65aaee6d",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\.conda\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1228 training claims.\n",
      "Loaded 154 dev claims.\n",
      "Loaded 1208827 evidences.\n",
      "Loaded 153 test claims.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing evidence: 100%|██████████| 1208827/1208827 [00:06<00:00, 179823.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence texts preprocessed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 下载停用词\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# 数据路径\n",
    "train_claims_path = 'data_cleaned_digital/train-claims.json'\n",
    "dev_claims_path = 'data_cleaned_digital/dev-claims.json'\n",
    "evidence_path = 'data_cleaned_digital/evidence.json'\n",
    "test_claims_path = 'data_cleaned_digital/test-claims-unlabelled.json'\n",
    "\n",
    "# 加载数据\n",
    "with open(train_claims_path, 'r', encoding='utf-8') as f:\n",
    "    train_claims = json.load(f)\n",
    "with open(dev_claims_path, 'r', encoding='utf-8') as f:\n",
    "    dev_claims = json.load(f)\n",
    "with open(evidence_path, 'r', encoding='utf-8') as f:\n",
    "    evidences = json.load(f) # evidences 是一个 dict {evidence_id: evidence_text}\n",
    "with open(test_claims_path, 'r', encoding='utf-8') as f:\n",
    "    test_claims = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(train_claims)} training claims.\")\n",
    "print(f\"Loaded {len(dev_claims)} dev claims.\")\n",
    "print(f\"Loaded {len(evidences)} evidences.\")\n",
    "print(f\"Loaded {len(test_claims)} test claims.\")\n",
    "\n",
    "# 准备证据列表和ID列表，用于BM25和后续步骤\n",
    "evidence_ids = list(evidences.keys())\n",
    "evidence_texts = list(evidences.values())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 简单的文本预处理：小写，去除停用词 (可根据需要扩展)\n",
    "    tokens = text.lower().split()\n",
    "    return [word for word in tokens if word not in english_stopwords]\n",
    "\n",
    "processed_evidence_texts = [preprocess_text(text) for text in tqdm(evidence_texts, desc=\"Preprocessing evidence\")]\n",
    "print(\"Evidence texts preprocessed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e71b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64072734",
   "metadata": {
    "id": "63c0e8c3",
    "language": "markdown"
   },
   "source": [
    "## 2. 第一阶段：BM25 候选召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd9d0ec",
   "metadata": {
    "id": "d3024ef6",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 model initialized.\n",
      "BM25 candidates for dev claim 'claim-752': 100 evidences\n",
      "['evidence-67732', 'evidence-572512', 'evidence-684667', 'evidence-452156', 'evidence-554677']\n"
     ]
    }
   ],
   "source": [
    "# 初始化 BM25 模型\n",
    "bm25 = BM25Okapi(processed_evidence_texts)\n",
    "print(\"BM25 model initialized.\")\n",
    "\n",
    "def retrieve_bm25(claim_text, bm25_model, evidence_id_list, top_n=100):\n",
    "    processed_claim = preprocess_text(claim_text)\n",
    "    scores = bm25_model.get_scores(processed_claim)\n",
    "    top_n_indices = np.argsort(scores)[::-1][:top_n]\n",
    "    return [evidence_id_list[i] for i in top_n_indices]\n",
    "\n",
    "# 示例：为一个dev claim获取BM25候选\n",
    "if dev_claims: #确保dev_claims不为空\n",
    "    sample_claim_id_bm25 = list(dev_claims.keys())[0]\n",
    "    sample_claim_text_bm25 = dev_claims[sample_claim_id_bm25]['claim_text']\n",
    "    bm25_candidates = retrieve_bm25(sample_claim_text_bm25, bm25, evidence_ids, top_n=100)\n",
    "    print(f\"BM25 candidates for dev claim '{sample_claim_id_bm25}': {len(bm25_candidates)} evidences\")\n",
    "    print(bm25_candidates[:5]) # 打印前5个候选\n",
    "else:\n",
    "    print(\"Dev claims are empty, skipping BM25 example.\")\n",
    "    bm25_candidates = [] # 为后续步骤提供空列表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02161abf",
   "metadata": {
    "id": "61ba95ee",
    "language": "markdown"
   },
   "source": [
    "## 3. 第二阶段：Sentence-BERT 稠密检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bffc738",
   "metadata": {
    "id": "8ba13046",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-BERT model 'all-MiniLM-L6-v2' loaded.\n",
      "SBERT candidates after re-ranking BM25 results for dev claim 'claim-752': 50 evidences\n",
      "['evidence-572512', 'evidence-67732', 'evidence-452156', 'evidence-780332', 'evidence-1000365']\n"
     ]
    }
   ],
   "source": [
    "# 加载Sentence-BERT模型\n",
    "sbert_model_name = 'all-MiniLM-L6-v2' # 可以选择其他预训练模型\n",
    "sbert_model = SentenceTransformer(sbert_model_name)\n",
    "print(f\"Sentence-BERT model '{sbert_model_name}' loaded.\")\n",
    "\n",
    "def retrieve_sbert(claim_text, candidate_ids, all_evidences_dict, sbert_model, top_n=50):\n",
    "    if not candidate_ids:\n",
    "        return []\n",
    "    candidate_texts = [all_evidences_dict[eid] for eid in candidate_ids if eid in all_evidences_dict]\n",
    "    valid_candidate_ids = [eid for eid in candidate_ids if eid in all_evidences_dict]\n",
    "    if not valid_candidate_ids: # 如果过滤后没有有效候选\n",
    "        return []\n",
    "    \n",
    "    claim_embedding = sbert_model.encode(claim_text, convert_to_tensor=True)\n",
    "    candidate_embeddings = sbert_model.encode(candidate_texts, convert_to_tensor=True, batch_size=32)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    cosine_scores = util.pytorch_cos_sim(claim_embedding, candidate_embeddings)[0]\n",
    "    \n",
    "    # 获取得分最高的候选\n",
    "    top_results_indices = torch.topk(cosine_scores, k=min(top_n, len(valid_candidate_ids))).indices.tolist()\n",
    "    return [valid_candidate_ids[i] for i in top_results_indices]\n",
    "\n",
    "# 示例：对BM25的候选进行SBERT重排\n",
    "if dev_claims and bm25_candidates: #确保dev_claims和bm25_candidates不为空\n",
    "    sbert_candidates = retrieve_sbert(sample_claim_text_bm25, bm25_candidates, evidences, sbert_model, top_n=50)\n",
    "    print(f\"SBERT candidates after re-ranking BM25 results for dev claim '{sample_claim_id_bm25}': {len(sbert_candidates)} evidences\")\n",
    "    print(sbert_candidates[:5])\n",
    "else:\n",
    "    print(\"Skipping SBERT example due to empty dev claims or BM25 candidates.\")\n",
    "    sbert_candidates = [] # 为后续步骤提供空列表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2a2c4",
   "metadata": {
    "id": "4fca9a96",
    "language": "markdown"
   },
   "source": [
    "## 4. 第三阶段：BERT Cross-Encoder 重排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678cbb0d",
   "metadata": {
    "id": "1b236c64",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Encoder model 'cross-encoder/ms-marco-MiniLM-L-6-v2' loaded.\n",
      "Final top 5 evidences for dev claim 'claim-752':\n",
      "1. evidence-572512: south australia have the high power price in the world...\n",
      "2. evidence-67732: citation need south australia have the high retail price for electricity in the country...\n",
      "3. evidence-723533: accord to a sierra club analysis the us kemper project which be due to be online in 2017 be the most...\n",
      "4. evidence-780332: industrialise country such as canada the us and australia be among the high per capita consumer of e...\n",
      "5. evidence-622374: in australian state of south australia wind power champion by premier mike rann 20022011 now compris...\n"
     ]
    }
   ],
   "source": [
    "# 加载Cross-Encoder模型\n",
    "cross_encoder_model_name = 'cross-encoder/ms-marco-MiniLM-L-6-v2' # 适用于重排序任务\n",
    "cross_encoder_tokenizer = AutoTokenizer.from_pretrained(cross_encoder_model_name)\n",
    "cross_encoder_model = AutoModelForSequenceClassification.from_pretrained(cross_encoder_model_name)\n",
    "cross_encoder_model.eval() # 设置为评估模式\n",
    "print(f\"Cross-Encoder model '{cross_encoder_model_name}' loaded.\")\n",
    "\n",
    "def rerank_cross_encoder(claim_text, candidate_ids, all_evidences_dict, tokenizer, model, top_n=5):\n",
    "    if not candidate_ids:\n",
    "        return []\n",
    "    candidate_texts = [all_evidences_dict[eid] for eid in candidate_ids if eid in all_evidences_dict]\n",
    "    valid_candidate_ids = [eid for eid in candidate_ids if eid in all_evidences_dict]\n",
    "    if not valid_candidate_ids: # 如果过滤后没有有效候选\n",
    "        return []\n",
    "    \n",
    "    # 创建句子对\n",
    "    sentence_pairs = [[claim_text, cand_text] for cand_text in candidate_texts]\n",
    "    \n",
    "    # 对句子对进行打分\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentence_pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        # 确保输入在模型期望的设备上 (如果GPU可用)\n",
    "        # inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        scores = model(**inputs).logits.squeeze(-1) # 获取相关性得分, squeeze dim 1\n",
    "        if scores.ndim == 0: # 如果只有一个候选，scores可能是一个标量\n",
    "            scores = scores.unsqueeze(0)\n",
    "            \n",
    "    # 排序并选择top_n\n",
    "    top_indices = torch.topk(scores, k=min(top_n, len(valid_candidate_ids))).indices.tolist()\n",
    "    return [valid_candidate_ids[i] for i in top_indices]\n",
    "\n",
    "# 示例：对SBERT的候选进行Cross-Encoder重排\n",
    "if dev_claims and sbert_candidates: #确保dev_claims和sbert_candidates不为空\n",
    "    final_top_5_evidences = rerank_cross_encoder(sample_claim_text_bm25, sbert_candidates, evidences, cross_encoder_tokenizer, cross_encoder_model, top_n=5)\n",
    "    print(f\"Final top 5 evidences for dev claim '{sample_claim_id_bm25}':\")\n",
    "    for i, eid in enumerate(final_top_5_evidences):\n",
    "        print(f\"{i+1}. {eid}: {evidences.get(eid, 'Evidence not found')[:100]}...\")\n",
    "else:\n",
    "    print(\"Skipping Cross-Encoder example due to empty dev claims or SBERT candidates.\")\n",
    "    final_top_5_evidences = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390703b6",
   "metadata": {
    "id": "1c96e1ed",
    "language": "markdown"
   },
   "source": [
    "## 5. 最终输出Top 5 Evidence 并计算准确率 (针对Dev集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92e5a4a",
   "metadata": {
    "id": "1e531dd2",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipeline: Processing dev claims:   1%|          | 1/154 [00:06<15:49,  6.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m claim_text = claim_data[\u001b[33m'\u001b[39m\u001b[33mclaim_text\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 1. BM25\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m bm25_cand = \u001b[43mretrieve_bm25\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclaim_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbm25\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevidence_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bm25_cand:\n\u001b[32m     41\u001b[39m     dev_set_predictions_pipeline[claim_id] = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mretrieve_bm25\u001b[39m\u001b[34m(claim_text, bm25_model, evidence_id_list, top_n)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_bm25\u001b[39m(claim_text, bm25_model, evidence_id_list, top_n=\u001b[32m100\u001b[39m):\n\u001b[32m      6\u001b[39m     processed_claim = preprocess_text(claim_text)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     scores = \u001b[43mbm25_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_claim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     top_n_indices = np.argsort(scores)[::-\u001b[32m1\u001b[39m][:top_n]\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [evidence_id_list[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_n_indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\.conda\\envs\\nlp\\Lib\\site-packages\\rank_bm25.py:119\u001b[39m, in \u001b[36mBM25Okapi.get_scores\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[32m    118\u001b[39m     q_freq = np.array([(doc.get(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.doc_freqs])\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     score += (\u001b[38;5;28mself\u001b[39m.idf.get(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m) * (q_freq * (\u001b[38;5;28mself\u001b[39m.k1 + \u001b[32m1\u001b[39m) /\n\u001b[32m    120\u001b[39m                                        (q_freq + \u001b[38;5;28mself\u001b[39m.k1 * (\u001b[32m1\u001b[39m - \u001b[38;5;28mself\u001b[39m.b + \u001b[38;5;28mself\u001b[39m.b * doc_len / \u001b[38;5;28mself\u001b[39m.avgdl)))\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def evaluate_retrieval(predictions_dict, gold_claims_dict, top_k=5):\n",
    "    total_hits = 0\n",
    "    total_relevant_in_gold = 0\n",
    "    total_predicted = 0\n",
    "    num_claims_evaluated = 0\n",
    "\n",
    "    for claim_id, predicted_eids_list in predictions_dict.items():\n",
    "        if claim_id not in gold_claims_dict or not gold_claims_dict[claim_id].get('evidences'):\n",
    "            # print(f\"Skipping claim {claim_id}: No gold evidences found.\")\n",
    "            continue\n",
    "        \n",
    "        num_claims_evaluated += 1\n",
    "        gold_eids_set = set(gold_claims_dict[claim_id]['evidences'])\n",
    "        # Ensure predicted_eids_list is a list, even if it's empty\n",
    "        current_predicted_eids = predicted_eids_list if isinstance(predicted_eids_list, list) else []\n",
    "        predicted_eids_top_k_set = set(current_predicted_eids[:top_k])\n",
    "        \n",
    "        hits = len(gold_eids_set.intersection(predicted_eids_top_k_set))\n",
    "        total_hits += hits\n",
    "        total_relevant_in_gold += len(gold_eids_set)\n",
    "        total_predicted += len(predicted_eids_top_k_set) # Count unique predicted items up to K\n",
    "        \n",
    "    precision_at_k = total_hits / total_predicted if total_predicted > 0 else 0\n",
    "    recall_at_k = total_hits / total_relevant_in_gold if total_relevant_in_gold > 0 else 0 \n",
    "    f1_at_k = (2 * precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) if (precision_at_k + recall_at_k) > 0 else 0\n",
    "    \n",
    "    print(f\"Evaluated on {num_claims_evaluated} claims (out of {len(gold_claims_dict)} total gold claims).\")\n",
    "    print(f\"Precision@{top_k}: {precision_at_k:.4f}\")\n",
    "    print(f\"Recall@{top_k}:    {recall_at_k:.4f}\")\n",
    "    print(f\"F1-score@{top_k}:  {f1_at_k:.4f}\")\n",
    "    return precision_at_k, recall_at_k, f1_at_k\n",
    "\n",
    "# 对dev集进行完整评估\n",
    "dev_set_predictions_pipeline = {}\n",
    "if dev_claims: # 确保dev_claims不为空\n",
    "    for claim_id, claim_data in tqdm(dev_claims.items(), desc=\"Pipeline: Processing dev claims\"):\n",
    "        claim_text = claim_data['claim_text']\n",
    "        # 1. BM25\n",
    "        bm25_cand = retrieve_bm25(claim_text, bm25, evidence_ids, top_n=100)\n",
    "        if not bm25_cand:\n",
    "            dev_set_predictions_pipeline[claim_id] = []\n",
    "            continue\n",
    "        # 2. SBERT\n",
    "        sbert_cand = retrieve_sbert(claim_text, bm25_cand, evidences, sbert_model, top_n=50)\n",
    "        if not sbert_cand:\n",
    "            dev_set_predictions_pipeline[claim_id] = []\n",
    "            continue\n",
    "        # 3. Cross-Encoder\n",
    "        final_eids = rerank_cross_encoder(claim_text, sbert_cand, evidences, cross_encoder_tokenizer, cross_encoder_model, top_n=5)\n",
    "        dev_set_predictions_pipeline[claim_id] = final_eids\n",
    "\n",
    "    print(\"\\n--- Evaluation on Dev Set (Pipeline Top 5) ---\")\n",
    "    precision_dev, recall_dev, f1_dev = evaluate_retrieval(dev_set_predictions_pipeline, dev_claims, top_k=5)\n",
    "\n",
    "    # 输出一个dev集的预测示例\n",
    "    if dev_set_predictions_pipeline: # 确保有预测结果\n",
    "        example_claim_id_dev_eval = list(dev_set_predictions_pipeline.keys())[0] \n",
    "        print(f\"\\nExample prediction for dev claim '{example_claim_id_dev_eval}':\")\n",
    "        print(f\"Claim text: {dev_claims[example_claim_id_dev_eval]['claim_text']}\")\n",
    "        print(f\"Predicted evidence IDs: {dev_set_predictions_pipeline[example_claim_id_dev_eval]}\")\n",
    "        print(\"Predicted evidence texts:\")\n",
    "        for i, eid in enumerate(dev_set_predictions_pipeline[example_claim_id_dev_eval]):\n",
    "            print(f\"  {i+1}. {eid}: {evidences.get(eid, 'Evidence ID not found')[:150]}...\")\n",
    "        if dev_claims[example_claim_id_dev_eval].get('evidences'):\n",
    "            print(f\"Gold evidence IDs: {dev_claims[example_claim_id_dev_eval]['evidences']}\")\n",
    "else:\n",
    "    print(\"Dev claims data is empty. Skipping dev set evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86037c",
   "metadata": {
    "id": "97873f3f",
    "language": "markdown"
   },
   "source": [
    "## 6. 生成测试集预测文件 (可选)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f7b123",
   "metadata": {
    "id": "705c29e9",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipeline: Processing test claims: 100%|██████████| 153/153 [07:09<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set predictions (pipeline) saved to test_predictions_pipeline_final.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 对test集进行预测 (与dev集流程类似，但不进行评估，因为没有标签)\n",
    "test_set_final_predictions = {}\n",
    "if test_claims: # 确保test_claims不为空\n",
    "    for claim_id, claim_data in tqdm(test_claims.items(), desc=\"Pipeline: Processing test claims\"):\n",
    "        claim_text = claim_data['claim_text']\n",
    "        bm25_cand = retrieve_bm25(claim_text, bm25, evidence_ids, top_n=100)\n",
    "        if not bm25_cand:\n",
    "            test_set_final_predictions[claim_id] = []\n",
    "            continue\n",
    "        sbert_cand = retrieve_sbert(claim_text, bm25_cand, evidences, sbert_model, top_n=50)\n",
    "        if not sbert_cand:\n",
    "            test_set_final_predictions[claim_id] = []\n",
    "            continue\n",
    "        final_eids = rerank_cross_encoder(claim_text, sbert_cand, evidences, cross_encoder_tokenizer, cross_encoder_model, top_n=5)\n",
    "        test_set_final_predictions[claim_id] = final_eids\n",
    "\n",
    "    # 保存测试集预测结果到JSON文件\n",
    "    output_test_predictions_path = 'test_predictions_pipeline_final.json'\n",
    "    with open(output_test_predictions_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(test_set_final_predictions, f, indent=4)\n",
    "    print(f\"\\nTest set predictions (pipeline) saved to {output_test_predictions_path}\")\n",
    "else:\n",
    "    print(\"\\nTest claims data is empty. Skipping test set prediction generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d14fb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换后的格式示例:\n",
      "{\n",
      "    \"claim-752\": {\n",
      "        \"claim_text\": \"south australia have the most expensive electricity in the world\",\n",
      "        \"claim_label\": \"SUPPORTS\",\n",
      "        \"evidences\": [\n",
      "            \"evidence-572512\",\n",
      "            \"evidence-67732\",\n",
      "            \"evidence-723533\",\n",
      "            \"evidence-780332\",\n",
      "            \"evidence-622374\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "\n",
      "Dev集格式化预测结果已保存至: dev_predictions_formatted.json\n",
      "\n",
      "Test集格式化预测结果已保存至: test-output.json\n"
     ]
    }
   ],
   "source": [
    "# 7. 转换输出格式为指定结构\n",
    "\n",
    "def convert_prediction_format(predictions_dict, claims_dict):\n",
    "    \"\"\"\n",
    "    将预测结果转换为指定的输出格式\n",
    "    \n",
    "    Args:\n",
    "        predictions_dict: 当前格式的预测结果，如 {\"claim-1266\": [\"evidence-694262\", ...]}\n",
    "        claims_dict: 包含claim文本的字典\n",
    "    \n",
    "    Returns:\n",
    "        转换后的预测结果字典\n",
    "    \"\"\"\n",
    "    formatted_predictions = {}\n",
    "\n",
    "    for claim_id, evidence_list in predictions_dict.items():\n",
    "        # 确保claim_id在claims_dict中\n",
    "        if claim_id in claims_dict:\n",
    "            claim_text = claims_dict[claim_id]['claim_text']\n",
    "\n",
    "            # 创建新格式的数据结构\n",
    "            formatted_predictions[claim_id] = {\n",
    "                \"claim_text\": claim_text,\n",
    "                \"claim_label\": \"SUPPORTS\",  # 按要求统一设置为SUPPORTS\n",
    "                \"evidences\": evidence_list\n",
    "            }\n",
    "\n",
    "    return formatted_predictions\n",
    "\n",
    "\n",
    "# 转换dev集的预测结果\n",
    "if 'dev_set_predictions_pipeline' in locals() and dev_claims:\n",
    "    formatted_dev_predictions = convert_prediction_format(\n",
    "        dev_set_predictions_pipeline, dev_claims)\n",
    "\n",
    "    # 打印示例查看结果\n",
    "    if formatted_dev_predictions:\n",
    "        print(\"转换后的格式示例:\")\n",
    "        example_claim_id = list(formatted_dev_predictions.keys())[0]\n",
    "        print(json.dumps(\n",
    "            {example_claim_id: formatted_dev_predictions[example_claim_id]}, indent=4, ensure_ascii=False))\n",
    "\n",
    "        # 保存转换后的dev预测结果\n",
    "        output_formatted_dev_path = 'dev_predictions_formatted.json'\n",
    "        with open(output_formatted_dev_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(formatted_dev_predictions, f,\n",
    "                      indent=4, ensure_ascii=False)\n",
    "        print(f\"\\nDev集格式化预测结果已保存至: {output_formatted_dev_path}\")\n",
    "\n",
    "# 转换test集的预测结果\n",
    "if 'test_set_final_predictions' in locals() and test_claims:\n",
    "    formatted_test_predictions = convert_prediction_format(\n",
    "        test_set_final_predictions, test_claims)\n",
    "\n",
    "    # 保存转换后的test预测结果\n",
    "    output_formatted_test_path = 'test-output.json'\n",
    "    with open(output_formatted_test_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_test_predictions, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"\\nTest集格式化预测结果已保存至: {output_formatted_test_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
